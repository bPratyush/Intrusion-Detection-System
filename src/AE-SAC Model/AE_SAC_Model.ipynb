{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fe700b5e-389d-4abd-8779-aa526a8683c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Training Dataset: (125972, 42)\n",
      "Shape of Testing Dataset: (22543, 42)\n",
      "Label distribution Training set:\n",
      "label\n",
      "normal             67343\n",
      "neptune            41214\n",
      "satan               3633\n",
      "ipsweep             3599\n",
      "portsweep           2931\n",
      "smurf               2646\n",
      "nmap                1493\n",
      "back                 956\n",
      "teardrop             892\n",
      "warezclient          890\n",
      "pod                  201\n",
      "guess_passwd          53\n",
      "buffer_overflow       30\n",
      "warezmaster           20\n",
      "land                  18\n",
      "imap                  11\n",
      "rootkit               10\n",
      "loadmodule             9\n",
      "ftp_write              8\n",
      "multihop               7\n",
      "phf                    4\n",
      "perl                   3\n",
      "spy                    2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Label distribution Test set:\n",
      "label\n",
      "normal             9711\n",
      "neptune            4657\n",
      "guess_passwd       1231\n",
      "mscan               996\n",
      "warezmaster         944\n",
      "apache2             737\n",
      "satan               735\n",
      "processtable        685\n",
      "smurf               665\n",
      "back                359\n",
      "snmpguess           331\n",
      "saint               319\n",
      "mailbomb            293\n",
      "snmpgetattack       178\n",
      "portsweep           157\n",
      "ipsweep             141\n",
      "httptunnel          133\n",
      "nmap                 73\n",
      "pod                  41\n",
      "buffer_overflow      20\n",
      "multihop             18\n",
      "named                17\n",
      "ps                   15\n",
      "sendmail             14\n",
      "rootkit              13\n",
      "xterm                13\n",
      "teardrop             12\n",
      "xlock                 9\n",
      "land                  7\n",
      "xsnoop                4\n",
      "ftp_write             3\n",
      "worm                  2\n",
      "loadmodule            2\n",
      "perl                  2\n",
      "sqlattack             2\n",
      "udpstorm              2\n",
      "phf                   2\n",
      "imap                  1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "import time\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import linear_model\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.utils import resample\n",
    "from imblearn.datasets import fetch_datasets\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve, precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report, confusion_matrix\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split, StratifiedShuffleSplit\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "dataset_train=pd.read_csv('/Users/bpratyush/Downloads/NSL_KDD_Train.csv')\n",
    "dataset_test=pd.read_csv('/Users/bpratyush/Downloads/NSL_KDD_Test.csv')\n",
    "dataset_train.head()\n",
    "col_names = [\"duration\",\"protocol_type\",\"service\",\"flag\",\"src_bytes\",\n",
    "    \"dst_bytes\",\"land\",\"wrong_fragment\",\"urgent\",\"hot\",\"num_failed_logins\",\n",
    "    \"logged_in\",\"num_compromised\",\"root_shell\",\"su_attempted\",\"num_root\",\n",
    "    \"num_file_creations\",\"num_shells\",\"num_access_files\",\"num_outbound_cmds\",\n",
    "    \"is_host_login\",\"is_guest_login\",\"count\",\"srv_count\",\"serror_rate\",\n",
    "    \"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\n",
    "    \"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\",\n",
    "    \"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\n",
    "    \"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\n",
    "    \"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\",\"label\"]\n",
    "print(\"Shape of Training Dataset:\", dataset_train.shape)\n",
    "print(\"Shape of Testing Dataset:\", dataset_test.shape)\n",
    "dataset_train = pd.read_csv(\"/Users/bpratyush/Downloads/NSL_KDD_Train.csv\", header=None, names = col_names)\n",
    "dataset_test = pd.read_csv(\"/Users/bpratyush/Downloads/NSL_KDD_Test.csv\", header=None, names = col_names)\n",
    "print('Label distribution Training set:')\n",
    "print(dataset_train['label'].value_counts())\n",
    "print()\n",
    "print('Label distribution Test set:')\n",
    "print(dataset_test['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8a110c50-4a1a-4070-a5a5-cc6168472287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "epochs = 100\n",
    "batch_size = 180\n",
    "sample_size = 180\n",
    "learning_rate = 0.1\n",
    "tau = 0.001\n",
    "gamma = 0.2\n",
    "memory_size = 1000\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bd533ab0-4b5d-47f4-80ca-db808026cb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 100)\n",
    "        self.fc2 = nn.Linear(100, 100)\n",
    "        self.fc3 = nn.Linear(100, 100)\n",
    "        self.fc4 = nn.Linear(100, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        return torch.softmax(self.fc4(x), dim=-1)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 100)\n",
    "        self.fc2 = nn.Linear(100, 100)\n",
    "        self.fc3 = nn.Linear(100, 100)\n",
    "        self.fc4 = nn.Linear(100, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        return self.fc4(x)\n",
    "\n",
    "# Initialize the networks\n",
    "actor = Actor(122, 23)\n",
    "q_critic = Critic(122, 23)\n",
    "v_critic = Critic(122, 1)\n",
    "# Initialize the optimizers\n",
    "actor_optimizer = optim.Adam(actor.parameters(), lr=learning_rate)\n",
    "q_critic_optimizer = optim.Adam(q_critic.parameters(), lr=learning_rate)\n",
    "v_critic_optimizer = optim.Adam(v_critic.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7a047b20-0c2a-4a63-ab02-deb08f0d3fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvironmentalAgent:\n",
    "    def __init__(self, input_size, output_size, learning_rate):\n",
    "        self.actor = Actor(input_size, output_size)\n",
    "        self.q_critic = Critic(input_size, output_size)\n",
    "        self.v_critic = Critic(input_size, 1)\n",
    "\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=learning_rate)\n",
    "        self.q_critic_optimizer = optim.Adam(self.q_critic.parameters(), lr=learning_rate)\n",
    "        self.v_critic_optimizer = optim.Adam(self.v_critic.parameters(), lr=learning_rate)\n",
    "\n",
    "    def get_next_state(self, data, current_state, action):\n",
    "        # Find the index of the current state in the data\n",
    "        index = np.where(np.all(data == current_state, axis=1))[0][0]\n",
    "        \n",
    "        # Use the action to determine the index of the next state\n",
    "        next_index = (index + action) % len(data)\n",
    "        \n",
    "        # Get the next state\n",
    "        next_state = data[next_index]\n",
    "        \n",
    "        return next_state\n",
    "\n",
    "    def resample_data(self, data, labels, counts):\n",
    "        # Convert the data and labels to a pandas DataFrame\n",
    "        df = pd.DataFrame(data)\n",
    "        df['label'] = labels\n",
    "\n",
    "        # Resample each class to the specified count\n",
    "        resampled_df = pd.DataFrame()\n",
    "        for label, count in counts.items():\n",
    "            class_df = df[df['label'] == label]\n",
    "            resampled_class_df = class_df.sample(count, replace=True)\n",
    "            resampled_df = pd.concat([resampled_df, resampled_class_df])\n",
    "                   # Shuffle the resampled DataFrame\n",
    "            resampled_df = resampled_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "        # Convert the resampled DataFrame back into data and labels\n",
    "            resampled_data = resampled_df.drop('label', axis=1).values\n",
    "            resampled_labels = resampled_df['label'].values\n",
    "\n",
    "        return resampled_data, resampled_labels\n",
    "environmental_agent = EnvironmentalAgent(122, 23, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "87bad516-6f32-419e-b3d4-78944be6ec53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actor Model:\n",
      "Actor(\n",
      "  (fc1): Linear(in_features=122, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (fc3): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (fc4): Linear(in_features=100, out_features=23, bias=True)\n",
      ")\n",
      "\n",
      "Q-Critic Model:\n",
      "Critic(\n",
      "  (fc1): Linear(in_features=122, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (fc3): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (fc4): Linear(in_features=100, out_features=23, bias=True)\n",
      ")\n",
      "\n",
      "V-Critic Model:\n",
      "Critic(\n",
      "  (fc1): Linear(in_features=122, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (fc3): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (fc4): Linear(in_features=100, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(\"Actor Model:\")\n",
    "print(environmental_agent.actor)\n",
    "print(\"\\nQ-Critic Model:\")\n",
    "print(environmental_agent.q_critic)\n",
    "print(\"\\nV-Critic Model:\")\n",
    "print(environmental_agent.v_critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "72fd80e7-e915-4d4d-9f62-30066235d7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierAgent:\n",
    "    def __init__(self, input_size, output_size, learning_rate):\n",
    "        self.actor = Actor(input_size, output_size)\n",
    "        self.q_critic = Critic(input_size, output_size)\n",
    "        self.v_critic = Critic(input_size, 1)\n",
    "\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=learning_rate)\n",
    "        self.q_critic_optimizer = optim.Adam(self.q_critic.parameters(), lr=learning_rate)\n",
    "        self.v_critic_optimizer = optim.Adam(self.v_critic.parameters(), lr=learning_rate)\n",
    "classifier_agent = ClassifierAgent(122, 5, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3c2d6d9e-8ee7-4b66-9ce6-1ae3185e7ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actor Model:\n",
      "Actor(\n",
      "  (fc1): Linear(in_features=122, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (fc3): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (fc4): Linear(in_features=100, out_features=5, bias=True)\n",
      ")\n",
      "\n",
      "Q-Critic Model:\n",
      "Critic(\n",
      "  (fc1): Linear(in_features=122, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (fc3): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (fc4): Linear(in_features=100, out_features=5, bias=True)\n",
      ")\n",
      "\n",
      "V-Critic Model:\n",
      "Critic(\n",
      "  (fc1): Linear(in_features=122, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (fc3): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (fc4): Linear(in_features=100, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(\"Actor Model:\")\n",
    "print(classifier_agent.actor)\n",
    "print(\"\\nQ-Critic Model:\")\n",
    "print(classifier_agent.q_critic)\n",
    "print(\"\\nV-Critic Model:\")\n",
    "print(classifier_agent.v_critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ed8a2c64-00bd-4827-afaa-c4163f072aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_classifier_reward(predicted_label, actual_label, high_percentage_categories, low_percentage_categories):\n",
    "    if predicted_label == actual_label:\n",
    "        if actual_label in high_percentage_categories:\n",
    "            return 1\n",
    "        elif actual_label in low_percentage_categories:\n",
    "            return 2\n",
    "    return 0\n",
    "\n",
    "def calculate_environment_reward(predicted_label, actual_label, high_percentage_categories, low_percentage_categories):\n",
    "    if predicted_label != actual_label:\n",
    "        if actual_label in high_percentage_categories:\n",
    "            return 1\n",
    "        elif actual_label in low_percentage_categories:\n",
    "            return 2\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "130c5b20-7073-4436-9348-847bd91719be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of continuous features: 38\n",
      "Number of categorical features: 3\n",
      "label\n",
      "normal    67343\n",
      "dos       45927\n",
      "probe     11656\n",
      "r2l         995\n",
      "u2r          52\n",
      "Name: count, dtype: int64\n",
      "Total number of features: 41\n",
      "           normal    dos  probe   r2l   u2r\n",
      "Original    67343  45927  11656   995    52\n",
      "Resampled    4689   2789   2187  5795  3971\n"
     ]
    }
   ],
   "source": [
    "# Scale continuous features\n",
    "continuous_features = dataset_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "scaler = MinMaxScaler()\n",
    "dataset_train[continuous_features] = scaler.fit_transform(dataset_train[continuous_features])\n",
    "num_continuous_features = len(continuous_features)\n",
    "print(f\"Number of continuous features: {num_continuous_features}\")\n",
    "# One-hot encode categorical features\n",
    "categorical_features=['protocol_type', 'service', 'flag'] \n",
    "one_hot = OneHotEncoder()\n",
    "transformer = ColumnTransformer([(\"OneHot\", one_hot, categorical_features)], remainder='passthrough')\n",
    "num_categorical_features = len(categorical_features)\n",
    "print(f\"Number of categorical features: {num_categorical_features}\")\n",
    "# Define the mapping from specific attack types to general categories\n",
    "attack_mapping = {\n",
    "    'back': 'dos', 'land': 'dos', 'neptune': 'dos', 'pod': 'dos', 'smurf': 'dos', 'teardrop': 'dos', 'mailbomb': 'dos', 'apache2': 'dos', 'processtable': 'dos', 'udpstorm': 'dos',\n",
    "    'ipsweep': 'probe', 'nmap': 'probe', 'portsweep': 'probe', 'satan': 'probe', 'mscan': 'probe', 'saint': 'probe',\n",
    "    'ftp_write': 'r2l', 'guess_passwd': 'r2l', 'imap': 'r2l', 'multihop': 'r2l', 'phf': 'r2l', 'spy': 'r2l', 'warezclient': 'r2l', 'warezmaster': 'r2l', 'sendmail': 'r2l', 'named': 'r2l', 'snmpgetattack': 'r2l', 'snmpguess': 'r2l', 'xlock': 'r2l', 'xsnoop': 'r2l', 'worm': 'r2l',\n",
    "    'buffer_overflow': 'u2r', 'loadmodule': 'u2r', 'perl': 'u2r', 'rootkit': 'u2r', 'httptunnel': 'u2r', 'ps': 'u2r', 'sqlattack': 'u2r', 'xterm': 'u2r',\n",
    "    'normal': 'normal'\n",
    "}\n",
    "\n",
    "# Apply the mapping to the labels\n",
    "dataset_train['label'] = dataset_train['label'].map(attack_mapping)\n",
    "\n",
    "# Check the distribution of the general categories\n",
    "print(dataset_train['label'].value_counts())\n",
    "num_features = dataset_train.shape[1] - 1 #-1 for label\n",
    "print(f'Total number of features: {num_features}')\n",
    "data = dataset_train.drop('label', axis=1)\n",
    "labels = dataset_train['label']\n",
    "counts = {'normal': 4689, 'dos': 2789, 'probe': 2187, 'r2l': 5795, 'u2r': 3971}\n",
    "environmental_agent = EnvironmentalAgent(122, 23, learning_rate)\n",
    "resampled_data, resampled_labels = environmental_agent.resample_data(data, labels, counts)\n",
    "resampled_dataset = pd.DataFrame(resampled_data, columns=dataset_train.columns[:-1])\n",
    "resampled_dataset['label'] = resampled_labels\n",
    "import pandas as pd\n",
    "original_counts = dataset_train['label'].value_counts()\n",
    "resampled_counts = resampled_dataset['label'].value_counts()\n",
    "counts_df = pd.DataFrame({\n",
    "    'normal': [original_counts['normal'], resampled_counts['normal']],\n",
    "    'dos': [original_counts['dos'], resampled_counts['dos']],\n",
    "    'probe': [original_counts['probe'], resampled_counts['probe']],\n",
    "    'r2l': [original_counts['r2l'], resampled_counts['r2l']],\n",
    "    'u2r': [original_counts['u2r'], resampled_counts['u2r']],\n",
    "}, index=['Original', 'Resampled'])\n",
    "print(counts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "04468648-35cd-4818-8e57-ee855edcaae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAC:\n",
    "    def __init__(self, actor, q_critic, v_critic, actor_optimizer, q_critic_optimizer, v_critic_optimizer, gamma=0.99, tau=0.005):\n",
    "        self.actor = actor\n",
    "        self.q_critic = q_critic\n",
    "        self.v_critic = v_critic\n",
    "        self.actor_optimizer = actor_optimizer\n",
    "        self.q_critic_optimizer = q_critic_optimizer\n",
    "        self.v_critic_optimizer = v_critic_optimizer\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.memory_env = []\n",
    "        self.memory_class = []\n",
    "\n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        # Add to memory\n",
    "        self.memory_env.append((state, action, reward, next_state, done))\n",
    "        self.memory_class.append((state, action, reward, next_state, done))\n",
    "\n",
    "        # Sample a batch of experiences from memory\n",
    "        batch_env = random.sample(self.memory_env, min(len(self.memory_env), batch_size))\n",
    "        batch_class = random.sample(self.memory_class, min(len(self.memory_class), batch_size))\n",
    "\n",
    "        for state, action, reward, next_state, done in batch_env:\n",
    "            self.update_env(state, action, reward, next_state, done)\n",
    "\n",
    "        for state, action, reward, next_state, done in batch_class:\n",
    "            self.update_class(state, action, reward, next_state, done)\n",
    "\n",
    "    def update_env(self, state, action, reward, next_state, done):\n",
    "        # Update environment agent (Q critic)\n",
    "        with torch.no_grad():\n",
    "            next_action = self.actor(next_state)\n",
    "            q_next_state = self.q_critic(next_state, next_action)\n",
    "            q_target = reward + (1 - done) * self.gamma * q_next_state\n",
    "\n",
    "        q_current = self.q_critic(state, action)\n",
    "        q_loss = nn.MSELoss()(q_current, q_target)\n",
    "        self.q_critic_optimizer.zero_grad()\n",
    "        q_loss.backward()\n",
    "        self.q_critic_optimizer.step()\n",
    "\n",
    "    def update_class(self, state, action, reward, next_state, done):\n",
    "        # Update classifier agent (V critic and actor)\n",
    "        with torch.no_grad():\n",
    "            next_action = self.actor(next_state)\n",
    "            q_next_state = self.q_critic(next_state, next_action)\n",
    "            v_target = q_next_state\n",
    "\n",
    "        v_current = self.v_critic(state)\n",
    "        v_loss = nn.MSELoss()(v_current, v_target)\n",
    "        self.v_critic_optimizer.zero_grad()\n",
    "        v_loss.backward()\n",
    "        self.v_critic_optimizer.step()\n",
    "\n",
    "        action_pred = self.actor(state)\n",
    "        q_current = self.q_critic(state, action_pred)\n",
    "        actor_loss = -torch.mean(q_current)\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # Soft update of V critic\n",
    "        for target_param, param in zip(self.v_critic.parameters(), self.q_critic.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
